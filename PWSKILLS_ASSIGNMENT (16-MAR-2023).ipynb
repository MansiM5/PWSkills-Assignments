{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba570058",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how\n",
    "can they be mitigated?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa654375",
   "metadata": {},
   "source": [
    "Overfitting is when Train dataset has low bias and Test dataset has high variance.Underfitting is when Train dataset has high bias and Test dataset has high variance.\n",
    "\n",
    "The consequences are that they can lead to poor model performance. Overfitting means that the model is too sensitive to the training data and cannot generalize well to new data. Underfitting means that the model is too simple and cannot capture the complexities in the data.\n",
    "\n",
    "To mitigate overfitting, several techniques can be used, including regularization, cross-validation, and early stopping.To mitigate underfitting, several techniques can be used, including increasing the complexity of the model, adding more features to the data, and increasing the amount of training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d1dc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2: How can we reduce overfitting? Explain in brief."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47908eff",
   "metadata": {},
   "source": [
    "Overfitting can be reduced by using several techniques such as regularization, cross-validation, and early stopping. Regularization adds a penalty term to the loss function to prevent the model from fitting the training data too well. Cross-validation involves splitting the dataset into multiple subsets and training the model on different combinations of subsets to evaluate its performance on new data. Early stopping involves stopping the training process when the performance on the validation set starts to degrade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e988ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3: Explain underfitting. List scenarios where underfitting can occur in ML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5245cc",
   "metadata": {},
   "source": [
    "Underfitting is when Train dataset has high bias and Test dataset has high variance. It occurs when learning model is too simple and cannot capture the patterns of data.\n",
    "\n",
    "Scenarios:\n",
    "* Poor feature selection\n",
    "* Imbalance data\n",
    "* Insufficient training data\n",
    "* Wrong choice of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c144f9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and\n",
    "variance, and how do they affect model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93e871b",
   "metadata": {},
   "source": [
    "Bias-variance tradeoff in machine learning is a concept that refers to bias & variance of a model and to generalize new, unseen data.\n",
    "\n",
    "Bias refers to the error that is introduced by approximating a real-world problem with a simplified model. A model with high bias makes strong assumptions about the data and may oversimplify the problem, leading to underfitting.\n",
    "\n",
    "Variance refers to the error that is introduced by the model's sensitivity to small fluctuations in the training data. A model with high variance is too complex and captures the noise in the training data, leading to overfitting.\n",
    "\n",
    "The tradeoff between bias and variance occurs because decreasing one usually increases the other. A model with high bias and low variance may be too simple and fail to capture the complexities of the data, while a model with low bias and high variance may be too complex and overfit the training data.\n",
    "\n",
    "Models with high bias tend to have poor performance on both the training and test data, while models with high variance tend to have good performance on the training data but poor performance on the test data. The optimal model is one that has low bias and low variance and is able to generalize well to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4050b415",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models.\n",
    "How can you determine whether your model is overfitting or underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21b8465",
   "metadata": {},
   "source": [
    "Cross-validation: Cross-validation is a technique used to estimate the performance of a model on new, unseen data. It involves splitting the dataset into multiple subsets and training the model on different combinations of subsets to evaluate its performance on new data. If the model performs well on the training data but poorly on the validation data, it may be overfitting. If it performs poorly on both the training and validation data, it may be underfitting.\n",
    "\n",
    "Learning Curves: Learning curves are plots that show the performance of a model on the training and validation data as a function of the amount of training data. If the learning curve for the training data is much better than the one for the validation data, it may indicate that the model is overfitting. If both curves plateau at a low level of performance, it may indicate that the model is underfitting.\n",
    "\n",
    "Regularization: Regularization is a technique used to prevent overfitting by adding a penalty term to the loss function. If the regularization parameter is too high, it may lead to underfitting, and if it is too low, it may lead to overfitting.\n",
    "\n",
    "Feature Importance: Feature importance is a technique used to identify the most important features in a model. If the model is overfitting, it may assign high importance to noise or irrelevant features. If it is underfitting, it may assign low importance to important features.\n",
    "\n",
    "Residual Plots: Residual plots are plots of the difference between the predicted values and the actual values as a function of the predicted values. If the residuals are random and centered around zero, it may indicate that the model is well-calibrated. If the residuals show a pattern or trend, it may indicate that the model is overfitting or underfitting.\n",
    "\n",
    "To determine whether a model is overfitting or underfitting we need to evaluate its performance on both the training and validation data. If the model has high accuracy on the training data but poor accuracy on the validation data, it may be overfitting. If it has poor accuracy on both, it may be underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23363b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias\n",
    "and high variance models, and how do they differ in terms of their performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50749366",
   "metadata": {},
   "source": [
    "Bias refers to the error that is introduced by approximating a real-world problem with a simplified model. A model with high bias makes strong assumptions about the data and may oversimplify the problem, leading to underfitting.\n",
    "\n",
    "Variance refers to the error that is introduced by the model's sensitivity to small fluctuations in the training data. A model with high variance is too complex and captures the noise in the training data, leading to overfitting.\n",
    "\n",
    "A high bias model in this case would be a simple linear regression model that assumes a linear relationship between the size of the house and its price. This model may have low accuracy on both the training and test data, as it oversimplifies the problem and fails to capture the non-linear relationships between the features and the target.\n",
    "\n",
    "A high variance model in this case would be a complex polynomial regression model that captures all the fluctuations and noise in the training data. This model may have high accuracy on the training data, but perform poorly on the test data, as it fails to generalize to new data and captures the noise in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8207bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe\n",
    "some common regularization techniques and how they work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4c1b33",
   "metadata": {},
   "source": [
    "Regularization is a technique used to prevent overfitting by adding a penalty term to the loss function. If the regularization parameter is too low, it may lead to overfitting.\n",
    "\n",
    "L1 Regularization (Lasso): L1 regularization adds a penalty term equal to the absolute value of the coefficients to the loss function. This technique encourages the model to have sparse coefficients, as it shrinks some of them towards zero. L1 regularization is often used for feature selection, as it can drive some coefficients to exactly zero.\n",
    "\n",
    "L2 Regularization (Ridge): L2 regularization adds a penalty term equal to the square of the coefficients to the loss function. This technique encourages the model to have small coefficients, as it shrinks them towards zero. L2 regularization is often used for smoothing, as it can reduce the impact of noisy or irrelevant features.\n",
    "\n",
    "Elastic Net Regularization: Elastic Net regularization combines both L1 and L2 regularization by adding a penalty term that is a weighted combination of the two. This technique can be useful when there are many features with different degrees of relevance, as it can identify the most important features while also reducing the impact of noise or irrelevant features.\n",
    "\n",
    "Dropout Regularization: Dropout regularization is a technique used in deep learning to prevent overfitting by randomly dropping out some nodes in the network during training. This technique forces the network to learn redundant representations, which can improve its generalization performance.\n",
    "\n",
    "Early Stopping: Early stopping is a technique used to prevent overfitting by monitoring the performance of the model on the validation data during training. When the performance on the validation data stops improving, the training is stopped, and the model with the best validation performance is selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1b516b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
